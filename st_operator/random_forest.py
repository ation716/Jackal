"""
éšæœºæ£®æ—æ¨¡å‹
åˆæ­¥å®éªŒï¼Œæ•ˆæœä¸ä½³
"""
import time

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
from security_data import *
import matplotlib.pyplot as plt
import matplotlib.dates as mdates

warnings.filterwarnings('ignore')




class EnhancedAdaptivePredictor:
    """

    """

    def __init__(self, lookback_days=180):
        self.lookback_days = lookback_days
        self.scaler = StandardScaler()
        self.model = None
        self.feature_names = []
        self.prediction_history = []
        self.actual_history = []

    def calculate_enhanced_indicators(self, df):
        """è®¡ç®—å¢å¼ºçš„æŠ€æœ¯æŒ‡æ ‡"""
        # åŸæœ‰çš„æŠ€æœ¯æŒ‡æ ‡
        # print(df['close'])
        df['price_change'] = df[SecurityFileds.CLOSE.value].pct_change()
        df['momentum_5'] = df[SecurityFileds.CLOSE.value].pct_change(5)
        df['momentum_10'] = df[SecurityFileds.CLOSE.value].pct_change(10)
        df['volatility_5'] = df['price_change'].rolling(5).std()
        df['volatility_10'] = df['price_change'].rolling(10).std()
        df['high_low_ratio'] = (df[SecurityFileds.HIGH.value] - df[SecurityFileds.LOW.value]) / df[SecurityFileds.CLOSE.value]
        df['close_position'] = (df[SecurityFileds.CLOSE.value] - df[SecurityFileds.LOW.value]) / (df[SecurityFileds.HIGH.value] - df[SecurityFileds.LOW.value] + 1e-8)
        df['volume_ma_5'] = df[SecurityFileds.VOLUME.value].rolling(5).mean()
        df['volume_ratio'] = df[SecurityFileds.VOLUME.value] / df['volume_ma_5']

        # ğŸ”¥ æ–°å¢å…³é”®æŒ‡æ ‡ ğŸ”¥

        # 1. å†å²ä»·æ ¼ä½ç½®ç‰¹å¾
        df['historical_high'] = df[SecurityFileds.HIGH.value].expanding().max()  
        df['historical_low'] = df[SecurityFileds.LOW.value].expanding().min()  

        # ç›¸å¯¹äºå†å²æå€¼çš„ä½ç½®
        df['to_historical_high'] = df[SecurityFileds.CLOSE.value] / df['historical_high']  # è·å†å²é«˜ç‚¹çš„è·ç¦»
        df['to_historical_low'] = df[SecurityFileds.CLOSE.value] / df['historical_low']  # è·å†å²ä½ç‚¹çš„è·ç¦»
        df['in_historical_range'] = (df[SecurityFileds.CLOSE.value] - df['historical_low']) / (
                    df['historical_high'] - df['historical_low'] + 1e-8)

        # çªç ´ä¿¡å·
        df['near_historical_high'] = (df['to_historical_high'] > 0.95).astype(int)  # æ¥è¿‘å†å²é«˜ç‚¹
        df['near_historical_low'] = (df['to_historical_low'] < 1.05).astype(int)  # æ¥è¿‘å†å²ä½ç‚¹

        # 2. æ¢æ‰‹ç‡ç‰¹å¾
        # å‡è®¾æ€»è‚¡æœ¬ä¸º1äº¿è‚¡ï¼ˆæ‚¨éœ€è¦æ›¿æ¢ä¸ºçœŸå®å€¼ï¼‰
        # total_shares = 100000000
        # df['turnover_rate'] = df[SecurityFileds.VOLUME.value] / total_shares * 100  # å½“æ—¥æ¢æ‰‹ç‡(%)

        # æ¢æ‰‹ç‡åŠ¨é‡
        df['turnover_ma_5'] = df[SecurityFileds.TURNOVER_RATE.value ].rolling(5).mean()
        df['turnover_ratio'] = df[SecurityFileds.TURNOVER_RATE.value ] / df['turnover_ma_5']  # æ¢æ‰‹ç‡ç›¸å¯¹å¼ºåº¦
        df['turnover_momentum'] = df[SecurityFileds.TURNOVER_RATE.value ].pct_change(5)  # æ¢æ‰‹ç‡åŠ¨é‡

        # 3. ä»·æ ¼-æˆäº¤é‡-æ¢æ‰‹ç‡ååŒç‰¹å¾
        df['price_turnover_corr'] = df[SecurityFileds.CLOSE.value].rolling(10).corr(df[SecurityFileds.TURNOVER_RATE.value ])  # ä»·é‡ç›¸å…³æ€§
        df['high_volume_breakout'] = ((df[SecurityFileds.TURNOVER_RATE.value ] > df['turnover_ma_5'] * 1.5) &
                                      (df['to_historical_high'] > 0.9)).astype(int)  # é«˜æ¢æ‰‹çªç ´

        # ä»·æ ¼åœ¨å…³é”®ä½ç½®æ—¶çš„æ¢æ‰‹ç‡ç‰¹å¾
        df['turnover_at_resistance'] = df[SecurityFileds.TURNOVER_RATE.value ] * df['near_historical_high']  # é˜»åŠ›ä½æ¢æ‰‹
        df['turnover_at_support'] = df[SecurityFileds.TURNOVER_RATE.value ] * df['near_historical_low']  # æ”¯æ’‘ä½æ¢æ‰‹

        return df

    def calculate_cost_pressure_features(self, df):
        """è®¡ç®—æˆæœ¬å‹åŠ›ç‰¹å¾ï¼ˆåŸæœ‰ï¼‰"""
        cost_levels = [SecurityFileds.COST_5TH_PERCENTILE.value, SecurityFileds.COST_15TH_PERCENTILE.value, SecurityFileds.COST_50TH_PERCENTILE.value, SecurityFileds.COST_85TH_PERCENTILE.value, SecurityFileds.COST_95TH_PERCENTILE.value]

        for level in cost_levels:
            df[f'price_to_{level}'] = df[SecurityFileds.CLOSE.value] / df[level]
            df[f'above_{level}'] = (df[SecurityFileds.CLOSE.value] > df[level]).astype(int)

        df['cost_range_5_95'] = df[SecurityFileds.COST_95TH_PERCENTILE.value] - df[SecurityFileds.COST_5TH_PERCENTILE.value]
        df['cost_concentration'] = df['cost_range_5_95'] / df[SecurityFileds.WEIGHTED_AVERAGE_COST.value]

        # æˆæœ¬åˆ†å¸ƒç™¾åˆ†ä½
        conditions = [
            df[SecurityFileds.CLOSE.value] <= df[SecurityFileds.COST_5TH_PERCENTILE.value],
            (df[SecurityFileds.CLOSE.value] > df[SecurityFileds.COST_5TH_PERCENTILE.value]) & (df[SecurityFileds.CLOSE.value] <= df[SecurityFileds.COST_15TH_PERCENTILE.value]),
            (df[SecurityFileds.CLOSE.value] > df[SecurityFileds.COST_15TH_PERCENTILE.value]) & (df[SecurityFileds.CLOSE.value] <= df[SecurityFileds.COST_50TH_PERCENTILE.value]),
            (df[SecurityFileds.CLOSE.value] > df[SecurityFileds.COST_50TH_PERCENTILE.value]) & (df[SecurityFileds.CLOSE.value] <= df[SecurityFileds.COST_85TH_PERCENTILE.value]),
            (df[SecurityFileds.CLOSE.value] > df[SecurityFileds.COST_85TH_PERCENTILE.value]) & (df[SecurityFileds.CLOSE.value] <= df[SecurityFileds.COST_95TH_PERCENTILE.value]),
            df[SecurityFileds.CLOSE.value] > df[SecurityFileds.COST_95TH_PERCENTILE.value]
        ]
        choices = [0.025, 0.10, 0.325, 0.675, 0.90, 0.975]
        df['cost_percentile'] = np.select(conditions, choices, default=0.5)

        return df

    def calculate_market_sentiment(self, df):
        """è®¡ç®—å¸‚åœºæƒ…ç»ªç‰¹å¾ï¼ˆåŸæœ‰ï¼‰"""
        df['win_rate_momentum'] = df[SecurityFileds.WIN_RATE.value].pct_change(5)
        df['win_rate_ma'] = df[SecurityFileds.WIN_RATE.value].rolling(10).mean()
        df['price_win_correlation'] = df[SecurityFileds.CLOSE.value].rolling(10).corr(df[SecurityFileds.WIN_RATE.value])
        df['support_strength'] = (df[SecurityFileds.CLOSE.value] - df[SecurityFileds.COST_5TH_PERCENTILE.value]) / (df[SecurityFileds.COST_95TH_PERCENTILE.value] - df[SecurityFileds.COST_5TH_PERCENTILE.value] + 1e-8)
        df['resistance_strength'] = (df[SecurityFileds.COST_95TH_PERCENTILE.value] - df[SecurityFileds.CLOSE.value]) / (df[SecurityFileds.COST_95TH_PERCENTILE.value] - df[SecurityFileds.COST_5TH_PERCENTILE.value] + 1e-8)

        return df

    def prepare_features(self, df):
        """å‡†å¤‡æ‰€æœ‰ç‰¹å¾"""
        df = self.calculate_enhanced_indicators(df.copy())
        df = self.calculate_cost_pressure_features(df.copy())
        df = self.calculate_market_sentiment(df.copy())

        # é€‰æ‹©ç‰¹å¾åˆ—ï¼ˆåŒ…å«æ–°å¢ç‰¹å¾ï¼‰
        feature_columns = [
            # åŸæœ‰åŸºç¡€ç‰¹å¾
            'price_change', 'momentum_5', 'momentum_10', 'volatility_5', 'volatility_10',
            'high_low_ratio', 'close_position', 'volume_ratio',

            # ğŸ”¥ æ–°å¢å†å²ä»·æ ¼ç‰¹å¾
            'to_historical_high', 'to_historical_low', 'in_historical_range',
            'near_historical_high', 'near_historical_low',

            # ğŸ”¥ æ–°å¢æ¢æ‰‹ç‡ç‰¹å¾
            SecurityFileds.TURNOVER_RATE.value , 'turnover_ratio', 'turnover_momentum',
            'price_turnover_corr', 'high_volume_breakout',
            'turnover_at_resistance', 'turnover_at_support',

            # åŸæœ‰æˆæœ¬ç‰¹å¾
            SecurityFileds.COST_5TH_PERCENTILE.value, SecurityFileds.COST_15TH_PERCENTILE.value, SecurityFileds.COST_50TH_PERCENTILE.value,
            SecurityFileds.COST_85TH_PERCENTILE.value, SecurityFileds.COST_95TH_PERCENTILE.value, f'above_{SecurityFileds.COST_5TH_PERCENTILE.value}',
            f'above_{SecurityFileds.COST_15TH_PERCENTILE.value}', f'above_{SecurityFileds.COST_50TH_PERCENTILE.value}', f'above_{SecurityFileds.COST_85TH_PERCENTILE.value}', f'above_{SecurityFileds.COST_95TH_PERCENTILE.value}',
            'cost_concentration', 'cost_percentile',

            # åŸæœ‰æƒ…ç»ªç‰¹å¾
            'win_rate_momentum', 'win_rate_ma', 'price_win_correlation',
            'support_strength', 'resistance_strength'
        ]

        # ç›®æ ‡å˜é‡ï¼šä¸‹ä¸€æ—¥çš„æ”¶ç›˜ä»·
        df['target'] = df[SecurityFileds.CLOSE.value].shift(-1)

        return df[feature_columns], df['target']

    def initial_train(self, df):
        """åˆå§‹è®­ç»ƒ"""
        print("è¿›è¡Œå¢å¼ºç‰ˆæ¨¡å‹è®­ç»ƒ...")

        features, target = self.prepare_features(df.iloc[:self.lookback_days].copy())
        self.feature_names = features.columns.tolist()

        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        valid_idx = features.notna().all(axis=1) & target.notna()
        features = features[valid_idx]
        target = target[valid_idx]

        if len(features) < 50:
            raise ValueError("æœ‰æ•ˆè®­ç»ƒæ•°æ®ä¸è¶³")

        # æ ‡å‡†åŒ–ç‰¹å¾
        features_scaled = self.scaler.fit_transform(features)

        # è®­ç»ƒå¢å¼ºçš„éšæœºæ£®æ—æ¨¡å‹
        self.model = RandomForestRegressor(
            n_estimators=150,  # å¢åŠ æ ‘çš„æ•°é‡
            max_depth=12,  # å¢åŠ æ·±åº¦ä»¥æ•æ‰æ›´å¤æ‚å…³ç³»
            min_samples_split=3,  # å‡å°‘åˆ†è£‚è¦æ±‚
            max_features='sqrt',  # ç‰¹å¾é‡‡æ ·ç­–ç•¥
            random_state=42
        )
        self.model.fit(features_scaled, target)

        # ç‰¹å¾é‡è¦æ€§åˆ†æ
        feature_importance = pd.DataFrame({
            'feature': self.feature_names,
            'importance': self.model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("Top 10 é‡è¦ç‰¹å¾:")
        print(feature_importance.head(10))

        # åˆå§‹é¢„æµ‹è¯„ä¼°
        train_pred = self.model.predict(features_scaled)
        train_rmse = np.sqrt(mean_squared_error(target, train_pred))
        train_mae = mean_absolute_error(target, train_pred)

        print(f"å¢å¼ºè®­ç»ƒå®Œæˆ - è®­ç»ƒé›†RMSE: {train_rmse:.4f}, MAE: {train_mae:.4f}")

        return True

    # å…¶ä½™æ–¹æ³•ï¼ˆpredict_next_day, online_learning_updateç­‰ï¼‰ä¿æŒä¸å˜
    # ...

    def predict_next_day(self, current_data):
        """é¢„æµ‹ä¸‹ä¸€å¤©"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨initial_trainæ–¹æ³•")

        # å‡†å¤‡ç‰¹å¾
        features, _ = self.prepare_features(current_data)
        latest_features = features.iloc[[-1]].copy()

        # ç¡®ä¿ç‰¹å¾å®Œæ•´
        if latest_features.isna().any().any():
            print("è­¦å‘Šï¼šæœ€æ–°æ•°æ®åŒ…å«ç¼ºå¤±å€¼ï¼Œä½¿ç”¨å‰å€¼å¡«å……")
            latest_features = latest_features.ffill()

        # æ ‡å‡†åŒ–å¹¶é¢„æµ‹
        features_scaled = self.scaler.transform(latest_features)
        prediction = self.model.predict(features_scaled)[0]

        return prediction

    def online_learning_update(self, new_actual_price, current_data):
        """åœ¨çº¿å­¦ä¹ æ›´æ–°"""
        self.actual_history.append(new_actual_price)

        if len(self.actual_history) > 5:  # ç§¯ç´¯ä¸€å®šæ–°æ•°æ®åæ›´æ–°
            # å‡†å¤‡æ›´æ–°æ•°æ®
            update_start = max(0, len(current_data) - 10)  # æœ€è¿‘10å¤©æ•°æ®
            update_data = current_data.iloc[update_start:]

            features, target = self.prepare_features(update_data)
            valid_idx = features.notna().all(axis=1) & target.notna()
            features = features[valid_idx]
            target = target[valid_idx]

            if len(features) > 3:
                features_scaled = self.scaler.transform(features)

                # éƒ¨åˆ†æ‹Ÿåˆæ›´æ–°ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­å¯èƒ½éœ€è¦ä½¿ç”¨warm_startï¼‰
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šç”¨æ–°æ•°æ®é‡æ–°è®­ç»ƒ
                try:
                    self.model.fit(features_scaled, target)
                    print(f"æ¨¡å‹å·²æ›´æ–°ï¼Œä½¿ç”¨{len(features)}ä¸ªæ–°æ ·æœ¬")
                except:
                    print("æ¨¡å‹æ›´æ–°å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸæ¨¡å‹")

def plot_predictions_simple(df, predictions, actuals, dates, train_days=180):
    """
    ç®€æ´ç‰ˆé¢„æµ‹ç»“æœå¯è§†åŒ–
    """
    plt.figure(figsize=(14, 8))

    # ç»˜åˆ¶å®Œæ•´å†å²ä»·æ ¼
    plt.plot(df.index, df[SecurityFileds.CLOSE.value], label='his_price', color='blue', alpha=0.6, linewidth=1)

    # ç»˜åˆ¶é¢„æµ‹æœŸä»·æ ¼
    plt.plot(dates, actuals, label='act_price', color='green', linewidth=2, marker='s', markersize=4)
    plt.plot(dates, predictions, label='pred_price', color='red', linewidth=2, marker='o', markersize=4)

    # æ ‡è®°é¢„æµ‹å¼€å§‹ç‚¹
    prediction_start_date = df.index[train_days]
    plt.axvline(x=prediction_start_date, color='black', linestyle='--', alpha=0.8, label='prediction start')

    # å¡«å……åŒºåŸŸ
    plt.axvspan(df.index[0], prediction_start_date, alpha=0.1, color='gray', label='training period')
    plt.axvspan(prediction_start_date, df.index[-1], alpha=0.1, color='yellow', label='prediction period')

    plt.title('prediction result', fontsize=14, fontweight='bold')
    plt.ylabel('price')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.xticks(rotation=45)

    # è®¾ç½®æ—¥æœŸæ ¼å¼
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%m-%d'))
    plt.gca().xaxis.set_major_locator(mdates.WeekdayLocator(interval=2))

    plt.tight_layout()
    plt.show()
    time.sleep(2)

def run_prediction_simulation(full_data):
    """
    è¿è¡Œå®Œæ•´çš„é¢„æµ‹æ¨¡æ‹Ÿ
    """
    print("å¼€å§‹è¯åˆ¸ä»·æ ¼é¢„æµ‹æ¨¡æ‹Ÿ...")
    print(f"æ•°æ®æ€»é‡: {len(full_data)}å¤©")
    print(f"è®­ç»ƒæœŸ: 180å¤©, é¢„æµ‹æœŸ: {len(full_data) - 180}å¤©")

    # åˆå§‹åŒ–é¢„æµ‹å™¨
    predictor = EnhancedAdaptivePredictor(lookback_days=180)

    # åˆå§‹è®­ç»ƒ
    success = predictor.initial_train(full_data)
    if not success:
        print("åˆå§‹è®­ç»ƒå¤±è´¥")
        return

    # é€æ—¥é¢„æµ‹
    predictions = []
    actuals = []
    dates = []

    for day in range(180, len(full_data)):
        current_date = full_data.index[day]
        current_data_snapshot = full_data.iloc[:day]  # ç¬¬ 0 å¤©åˆ°ç¬¬ day-1 å¤©

        try:
            # é¢„æµ‹ä¸‹ä¸€å¤©
            pred_price = predictor.predict_next_day(current_data_snapshot)
            actual_price = full_data.iloc[day][SecurityFileds.CLOSE.value]

            predictions.append(pred_price)
            actuals.append(actual_price)
            dates.append(current_date)

            # è¾“å‡ºé¢„æµ‹ç»“æœ
            error = pred_price - actual_price
            error_pct = error / actual_price * 100

            print(f"æ—¥æœŸ: {full_data.iloc[current_date]['trade_date'].strftime('%Y%m%d')}")
            print(f"  é¢„æµ‹: {pred_price:.4f}, å®é™…: {actual_price:.4f}, è¯¯å·®: {error_pct:+.2f}%")

            # åœ¨çº¿å­¦ä¹ æ›´æ–°ï¼ˆæ¯5å¤©æˆ–è¯¯å·®è¾ƒå¤§æ—¶ï¼‰
            if len(predictions) % 5 == 0 or abs(error_pct) > 2.0:
                predictor.online_learning_update(actual_price, current_data_snapshot)

        except Exception as e:
            print(f"æ—¥æœŸ {current_date} é¢„æµ‹å¤±è´¥: {e}")
            continue
    # å†é¢„æµ‹ä¸€å¤©
    current_data_snapshot = full_data.iloc[:len(full_data)-1]
    pred_price = predictor.predict_next_day(current_data_snapshot)
    print(f"ä¸‹ä¸€æ—¥é¢„æµ‹: {pred_price:.4f}")
    # è¯„ä¼°æœ€ç»ˆæ€§èƒ½
    if len(predictions) > 0:
        change_pct_actual = (np.array(actuals[1:]) - np.array(actuals[:-1])) / np.array(actuals[:-1]) * 100
        change_pct_pred = (np.array(predictions[1:]) - np.array(predictions[:-1])) / np.array(predictions[:-1]) * 100
        final_rmse = np.sqrt(mean_squared_error(change_pct_actual, change_pct_pred))
        final_mae = mean_absolute_error(change_pct_actual, change_pct_pred)
        accuracy = np.mean(np.sign(np.array(predictions[1:]) - np.array(predictions[:-1])) ==
                           np.sign(np.array(actuals[1:]) - np.array(actuals[:-1]))) * 100

        print("\n" + "=" * 50)
        print("é¢„æµ‹æ¨¡æ‹Ÿå®Œæˆ")
        print(f"æœ€ç»ˆæ€§èƒ½è¯„ä¼°:")
        print(f"RMSE: {final_rmse:.4f}")
        print(f"MAE: {final_mae:.4f}")
        print(f"æ–¹å‘å‡†ç¡®ç‡: {accuracy:.2f}%")


        with open('random_forest.txt', 'w') as f:
            f.write("dates\tactual\tprediction\n")
            for i in range(len(actuals)):
                if i>0:
                    f.write("{:<10.5f}{:<10.5f}{:<10.5f}{:<10.5f}{:<10.5f}\n".format(dates[i], actuals[i], predictions[i], change_pct_actual[i-1], change_pct_pred[i-1]))
                else:
                    f.write("{:<10.5f}{:<10.5f}{:<10.5f}\n".format(dates[i], actuals[i], predictions[i]))
        plot_predictions_simple(df, predictions, actuals, dates, train_days=180)
        return {
            'predictions': predictions,
            'actuals': actuals,
            'dates': dates,
            'metrics': {
                'rmse': final_rmse,
                'mae': final_mae,
                'direction_accuracy': accuracy
            }
        }





    # ä½¿ç”¨ç¤ºä¾‹
    # results = run_prediction_simulation(df)
    # plot_predictions_simple(df, results['predictions'], results['actuals'], results['dates'])


# # ç¤ºä¾‹ä½¿ç”¨
# def example_usage():
#     """
#     ç¤ºä¾‹ï¼šå¦‚ä½•ä½¿ç”¨è¿™ä¸ªé¢„æµ‹ç³»ç»Ÿ
#     """
#     # å‡è®¾æ‚¨æœ‰ä¸€ä¸ªDataFrameï¼ŒåŒ…å«200å¤©çš„æ•°æ®ï¼Œåˆ—åå¦‚ä¸‹ï¼š
#     # ['å¼€ç›˜ä»·', 'æ”¶ç›˜ä»·', SecurityFileds.HIGH.value, SecurityFileds.LOW.value, SecurityFileds.VOLUME.value,
#     #  SecurityFileds.COST_5TH_PERCENTILE, SecurityFileds.COST_15TH_PERCENTILE, SecurityFileds.COST_50TH_PERCENTILE, SecurityFileds.COST_85TH_PERCENTILE, SecurityFileds.COST_95TH_PERCENTILE,
#     #  'åŠ æƒå¹³å‡æˆæœ¬', 'èƒœç‡']
#
#     # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º
#     np.random.seed(42)
#     dates = pd.date_range('2023-01-01', periods=200, freq='D')
#
#     # æ¨¡æ‹Ÿä»·æ ¼æ•°æ®ï¼ˆéšæœºæ¸¸èµ°ï¼‰
#     prices = [100]
#     for i in range(1, 200):
#         change = np.random.normal(0, 0.02)  # æ—¥å‡2%æ³¢åŠ¨
#         prices.append(prices[-1] * (1 + change))
#
#     mock_data = pd.DataFrame({
#         'å¼€ç›˜ä»·': prices,
#         'æ”¶ç›˜ä»·': prices,
#         SecurityFileds.HIGH.value: [p * 1.02 for p in prices],
#         SecurityFileds.LOW.value: [p * 0.98 for p in prices],
#         SecurityFileds.VOLUME.value: np.random.lognormal(10, 1, 200),
#         SecurityFileds.COST_5TH_PERCENTILE: [p * 0.85 for p in prices],
#         SecurityFileds.COST_15TH_PERCENTILE: [p * 0.90 for p in prices],
#         SecurityFileds.COST_50TH_PERCENTILE: [p * 0.95 for p in prices],
#         SecurityFileds.COST_85TH_PERCENTILE: [p * 1.05 for p in prices],
#         SecurityFileds.COST_95TH_PERCENTILE: [p * 1.10 for p in prices],
#         'åŠ æƒå¹³å‡æˆæœ¬': [p * 0.98 for p in prices],
#         SecurityFileds.WIN_RATE: np.random.uniform(0.3, 0.7, 200)
#     }, index=dates)
#
#     # è¿è¡Œé¢„æµ‹æ¨¡æ‹Ÿ
#     results = run_prediction_simulation(mock_data)
#
#     return results

# è¿è¡Œç¤ºä¾‹
# results = example_usage()

if __name__ == '__main__':
    code_list={
        # 'hmqc':'000572',
        # 'flm':'603686',
        # 'cjxc':'002171',
        # 'hfzg':'602122',
        # 'ycm':'002101',
        # 'lszz':'603169',
        # 'hjhs':'603616',
        # 'xggf':'600815',
        # 'hfzg':'603122',
        # 'smgf':'600810',
        # 'sdjt':'600734',
        # 'tytx':'002792',
        # 'lkfw':'002413'
        # 'hxsp':'002702'
        # 'dmgx':'002632',
        'dbjt':'600693'

    }
    # ts_code = "603122.SH"
    start_date = "20240924"
    end_date = "20251209"
    analyzer = ChipDistributionAnalyzer()
    counter=0
    for name,ts_code in code_list.items():

        df1 = analyzer.get_daily_ak(ts_code, start_date, end_date)
        df1 = df1.rename(columns={'æ¢æ‰‹ç‡': 'turnover_rate'})
        df2 = analyzer.get_stock_chip_distribution(ts_code, start_date, end_date)
        df3 = analyzer.get_daily_tu(ts_code, start_date, end_date)
        df2 = df2.iloc[::-1].reset_index(drop=True)
        df3 = df3.iloc[::-1].reset_index(drop=True)
        combined_df = pd.concat([df2, df3.iloc[:, 2:11], df1.iloc[:, 11:12]], axis=1)
        combined_df.to_csv(f'../results/stacks/{name}.csv',
                  index=False,  # ä¸ä¿å­˜ç´¢å¼•
                  encoding='utf_8_sig',  # æ”¯æŒä¸­æ–‡
                  sep=',')  # åˆ†éš”ç¬¦
        counter+=1
        if counter % 5 == 0:
            time.sleep(60)
    # df = pd.read_csv('data_demo.csv',
    #                  encoding='utf_8_sig',  # ä¸­æ–‡ç¼–ç 
    #                  parse_dates=['trade_date'])  # è§£ææ—¥æœŸåˆ—
    # # df['trade_date']=pd.to_datetime(df['trade_date'].astype(str), format='%Y%m%d')
    # results = run_prediction_simulation(df)